{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch import cuda, device\n",
    "from torch import nn\n",
    "from torch import optim, from_numpy, tensor\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_data(data_directory, split, classes, img_size, model_name):\n",
    "    '''\n",
    "    Read training images and classes into multi-dimensional array.\n",
    "    Images are compressed to img_size x img_size.\n",
    "    '''\n",
    "    split_data = [[], [], []]\n",
    "    classes_data = [[], [], []]\n",
    "\n",
    "    for spl_index, spl in enumerate(split):\n",
    "        spl_path = os.path.join(data_directory, spl)\n",
    "        for cla_index, cla in enumerate(classes):\n",
    "            path = os.path.join(data_directory, spl, cla)\n",
    "            if classes.index(cla) == 0:\n",
    "                class_num = np.array([[1.], [0.]])\n",
    "            else:\n",
    "                class_num = np.array([[0.], [1.]])\n",
    "            # class_num = classes.index(cla) # !!! replaced by if else statement\n",
    "            for img in os.listdir(path):\n",
    "                img_array = cv.imread(os.path.join(\n",
    "                    path, img), cv.IMREAD_GRAYSCALE)\n",
    "                img_array = cv.resize(img_array, (img_size, img_size))\n",
    "                if model_name == 'base':\n",
    "                    img_array = np.reshape(\n",
    "                        img_array, (img_array.shape[0], img_array.shape[1], 1))\n",
    "                elif model_name == 'torch':\n",
    "                    img_array = np.reshape(\n",
    "                        img_array, (1, img_array.shape[0], img_array.shape[1]))\n",
    "                img_array = img_array.astype(\"float32\") / 255\n",
    "                split_data[spl_index].append(img_array)\n",
    "                classes_data[spl_index].append(class_num)\n",
    "        print(spl_path, '(read', len(classes_data[spl_index]), 'images)')\n",
    "\n",
    "    for i in range(0, len(split_data)):\n",
    "        comb_list = list(zip(split_data[i], classes_data[i]))\n",
    "        random.shuffle(comb_list)\n",
    "        split_data[i], classes_data[i] = zip(*comb_list)\n",
    "\n",
    "    return split_data, classes_data\n",
    "\n",
    "\n",
    "def reshape_img(img):\n",
    "    '''\n",
    "    Reshape image to 3D array with third dimension as 1.\n",
    "    '''\n",
    "    return np.reshape(img, (img.shape[0], img.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '../data/'\n",
    "split = ['train', 'val', 'test']\n",
    "classes = ['NORMAL', 'PNEUMONIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train (read 3756 images)\n",
      "../data/val (read 417 images)\n",
      "../data/test (read 1043 images)\n"
     ]
    }
   ],
   "source": [
    "split_data, classes_data = read_training_data(data_directory, split, classes, 100, 'torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = split_data[0], classes_data[0]\n",
    "x_val, y_val = split_data[1], classes_data[1]\n",
    "x_test, y_test = split_data[2], classes_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... created DataLoader for train, val and test.\n",
      "train finished...\n",
      "out tensor([[-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947],\n",
      "        [-0.8285,  0.3947]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 32 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m, out)\n\u001b[1;32m     89\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(out\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m predicted \u001b[39m=\u001b[39m predicted\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     91\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpredicted\u001b[39m\u001b[39m'\u001b[39m, predicted)\n\u001b[1;32m     92\u001b[0m \u001b[39mall\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 32 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "class TorchCNN(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TorchCNN, self).__init__()\n",
    "        self.c1 = nn.Conv2d(1, 32, 3)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.c2 = nn.Conv2d(32, 64, 3)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.f1 = nn.Linear(64 * 23 * 23, 128)\n",
    "        self.sig3 = nn.Sigmoid()\n",
    "        self.f2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, fw):\n",
    "        '''\n",
    "        '''\n",
    "        fw = self.c1(fw)\n",
    "        fw = self.sig1(fw)\n",
    "        fw = self.pool1(fw)\n",
    "        fw = self.c2(fw)\n",
    "        fw = self.sig2(fw)\n",
    "        fw = self.pool2(fw)\n",
    "        fw = fw.view(fw.size(0), -1)\n",
    "        fw = self.f1(fw)\n",
    "        fw = self.sig3(fw)\n",
    "        fw = self.f2(fw)\n",
    "        return fw\n",
    "    \n",
    "def torch_cnn_prepare_data(split_data, classes_data, batch_size):\n",
    "    '''\n",
    "    '''\n",
    "    x_train, y_train = split_data[0], classes_data[0]\n",
    "    x_val, y_val = split_data[1], classes_data[1]\n",
    "    x_test, y_test = split_data[2], classes_data[2]\n",
    "\n",
    "    train_data = TensorDataset(tensor(x_train), tensor(y_train))\n",
    "    train_load = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_data = TensorDataset(tensor(x_val), tensor(y_val))\n",
    "    val_load = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "    test_data = TensorDataset(tensor(x_test), tensor(y_test))\n",
    "    test_load = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print('... created DataLoader for train, val and test.')\n",
    "\n",
    "    return train_load, val_load, test_load\n",
    "\n",
    "# def run_torch_cnn(split_data, classes_data, epochs, learning_rate, batch_size):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "train_load, val_load, test_load = torch_cnn_prepare_data(split_data, classes_data, batch_size)\n",
    "base_torch_model = TorchCNN()\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(base_torch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda.is_available() else \"cpu\")\n",
    "base_torch_model.to(device)\n",
    "\n",
    "#\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    base_torch_model.train()\n",
    "    loss_run = 0.0\n",
    "    for images, labels in train_load:\n",
    "        images, labels = images.to(device), labels.squeeze().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = base_torch_model(images)\n",
    "        loss = loss_func(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_run += loss.item()\n",
    "    loss_res = loss_run / len(train_load)\n",
    "    print('train finished...')\n",
    "    base_torch_model.eval()\n",
    "    true, all = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_load:\n",
    "            images, labels = images.to(device), labels.squeeze().to(device)\n",
    "            out = base_torch_model(images)\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            all += labels.size(0)\n",
    "            true += (predicted == labels.squeeze()).sum().item()\n",
    "    acc = true / all\n",
    "    print('acc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [3, 3, 3, 3], expected input[32, 100, 100, 1] to have 3 channels, but got 100 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m outputs \u001b[39m=\u001b[39m base_torch_model(images)\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m loss_func(outputs, labels)\n\u001b[1;32m      9\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniforge3/envs/icds/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[91], line 15\u001b[0m, in \u001b[0;36mTorchCNN.forward\u001b[0;34m(self, fw)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, fw):\n\u001b[0;32m---> 15\u001b[0m     fw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc1(fw)\n\u001b[1;32m     16\u001b[0m     fw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msig1(fw)\n\u001b[1;32m     17\u001b[0m     fw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(fw)\n",
      "File \u001b[0;32m~/miniforge3/envs/icds/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/icds/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniforge3/envs/icds/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3, 3, 3, 3], expected input[32, 100, 100, 1] to have 3 channels, but got 100 channels instead"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    base_torch_model.train()\n",
    "    loss_run = 0.0\n",
    "    for images, labels in train_load:\n",
    "        images, labels = images.to(device), labels.squeeze().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = base_torch_model(images)\n",
    "        loss = loss_func(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_run += loss.item()\n",
    "    loss_res = loss_run / len(train_load)\n",
    "\n",
    "    base_torch_model.eval()\n",
    "    true, all = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_load:\n",
    "            images, labels = images.to(device), labels.squeeze().to(device)\n",
    "            out = base_torch_model(images)\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            all += labels.size(0)\n",
    "            true += (predicted == labels.squeeze()).sum().item()\n",
    "    acc = true / all\n",
    "    print('acc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing loop\n",
    "base_torch_model.eval()\n",
    "true, all = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_load:\n",
    "        images, labels = images.to(device), labels.squeeze().to(device)\n",
    "        out = base_torch_model(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        all += labels.size(0)\n",
    "        true += (predicted == labels.squeeze()).sum().item()\n",
    "acc = true / all\n",
    "print('acc', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
