{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 5: Implementing Principal Component Analysis\n",
    "  \n",
    "### Authors \n",
    "- Hagen, Paula\n",
    "- Huhle, Paul Moritz\n",
    "- Int-Veen, Harpreet\n",
    "- Kaviapoor Esfahani, Sepideh\n",
    "- KÃ¼hnel, Paul (Group leader)\n",
    "- Schuster, Karoline\n",
    "\n",
    "\n",
    "### Outline\n",
    "1. Load and prepare data\n",
    "2. Investigate results with PCA libraries\n",
    "    - pca\n",
    "    - sklearn\n",
    "    - statsmodels\n",
    "3. Apply own PCA on data and compare with libraries\n",
    "4. Use results for classification\n",
    "5. Compare results with tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "\n",
    "# basics and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pca library methods\n",
    "from pca import pca as pca_pca\n",
    "from statsmodels.multivariate.pca import PCA as stm_pca\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA as skl_pca\n",
    "\n",
    "# for classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# own functions\n",
    "from dataimport import get_df_merged_with_labels\n",
    "from dataimport import random_sample, get_random_sample\n",
    "from data_cleaning import preprocessing, drop_insignificant_data, center_data, standardize_data\n",
    "from our_pca import our_pca, apply_components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "## 1. Load and prepare data\n",
    "In this section, we use our own functions from dataimport.py and data_cleaning.py. In the end, we will have a (labeled) test set and an (unlabeled) training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get full data\n",
    "df_labeled =  get_df_merged_with_labels()\n",
    "print(\"Labeled Data: \\n\", df_labeled.head(10))\n",
    "\n",
    "# Check if there aren't any missings\n",
    "print(\"Number of missings: \", df_labeled.isnull().sum().sum())\n",
    "\n",
    "# make unlabeled data frame\n",
    "if 'Class' in df_labeled.columns:\n",
    "    df_unlabeled = df_labeled.drop(['Class'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "## 2. Investigate results with PCA libraries\n",
    "In this section, we compare the PCA results for this data set obtained from three libraries:\n",
    "- pca\n",
    "- sklearn\n",
    "- statsmodels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with pca library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pca from \"pca\" library with default parameters\n",
    "model_pca_pca = pca_pca()\n",
    "results_pca = model_pca_pca.fit_transform(center_data(df_unlabeled))\n",
    "\n",
    "# Plot the explained variance (elbow plot)\n",
    "model_pca_pca.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 2 components\n",
    "skl_pca_2_components = skl_pca(n_components=2)\n",
    "\n",
    "# apply components on data\n",
    "data_on_2_components = pd.DataFrame(skl_pca_2_components.fit_transform(df_unlabeled))\n",
    "data_on_2_components.columns = ['pca1','pca2']\n",
    "\n",
    "# add cancer labels \n",
    "data_on_2_components['cancer_type'] = df_labeled['Class']\n",
    "data_on_2_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present the data on the 5 clusters using seaborn maps \n",
    "sns.scatterplot(x='pca1',y='pca2', hue = 'cancer_type',data=data_on_2_components)\n",
    "# Move the hue legend to the top right corner\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1), title=\"Cancer Type\", frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the explained by the two components\n",
    "explained_variance_ratio = skl_pca_2_components.explained_variance_ratio_\n",
    "print(\"Variance explained by first and second component: \", explained_variance_ratio)\n",
    "\n",
    "# Sum the elements of the array for total variance\n",
    "var_sum = np.sum(explained_variance_ratio)\n",
    "print(\"Total variance explained with 2 components: \", var_sum)\n",
    "\n",
    "# Same with 80% variance explained: 129 components needed (We omitted plotting, as it is equivalent)\n",
    "pca_80_varexpl = skl_pca(.80)\n",
    "df_pca_80 = pd.DataFrame(pca_80_varexpl.fit_transform(df_unlabeled))\n",
    "\n",
    "explained_variance_ratio2 = pca_80_varexpl.explained_variance_ratio_\n",
    "var_sum2 = np.sum(explained_variance_ratio2)\n",
    "print(f\"Total variance explained with {df_pca_80.shape[1]} components: \", var_sum2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with statsmodels library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION: This takes about 30-60 seconds to run\n",
    "\n",
    "# check for zero or near-zero variance features (necessary for statsmodels PCA to converge)\n",
    "variances = np.var(df_unlabeled, axis=0)\n",
    "near_zero_var_indices = np.where(variances < 1e-6)[0]\n",
    "if near_zero_var_indices.size > 0:\n",
    "    print(\"Features with near-zero variance found. Removing them...\")\n",
    "    stm_data = np.delete(df_unlabeled.values, near_zero_var_indices, axis=1)\n",
    "print(f\"{df_unlabeled.shape[1] - stm_data.shape[1]} features removed. Now {stm_data.shape[1]} features in dataframe.\")\n",
    "\n",
    "# perform PCA with two components. Centered Data works best\n",
    "stm_pca_2_components = stm_pca(center_data(stm_data), ncomp=2, standardize=False)\n",
    "\n",
    "# apply components on data\n",
    "data_on_2_components = pd.DataFrame(stm_pca_2_components.factors)\n",
    "\n",
    "# add labels\n",
    "data_on_2_components.columns = ['pca1','pca2']\n",
    "data_on_2_components['cancer_type'] = df_labeled['Class']\n",
    "data_on_2_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# present the data on the 5 clusters using seaborn maps\n",
    "sns.scatterplot(x='pca1',y='pca2',hue = 'cancer_type', data=data_on_2_components)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1), title=\"Cancer Type\", frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply own PCA on data and compare with libraries\n",
    "You can find our own PCA implementation in the file our_pca.py. Here we apply it to the genetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use own pca method on subset of the data (only 2000 columns)\n",
    "n_components = 2\n",
    "train = pd.DataFrame(center_data(stm_data)) # we use the stm_data so they can be compared\n",
    "train = random_sample(train, 801, 2000) \n",
    "own_pca = our_pca(train, n_components)\n",
    "own_eigenvalues, own_eigenvectors = own_pca\n",
    "own_eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the eigenvectors from statsmodels\n",
    "stm_eigenvectors = stm_pca(train, ncomp=2).eigenvecs\n",
    "\n",
    "# Check if the eigenvectors contain the same values\n",
    "check_df = abs(stm_eigenvectors - own_eigenvectors)\n",
    "print(\"Differences in eigenvectors: \\n\", check_df)\n",
    "\n",
    "# Check if values in the DataFrame are nearly zero\n",
    "is_close_to_zero = np.isclose(check_df, 0.0, atol=1e-5)\n",
    "print(\"Are all eigenvector differences close to zero? \", np.all(is_close_to_zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the eigenvectors from sklearn\n",
    "skl_eigenvectors = skl_pca(n_components=2).fit(train).components_\n",
    "skl_eigenvectors = np.transpose(skl_eigenvectors)\n",
    "\n",
    "# Check if the eigenvectors contain the same values\n",
    "check_df = abs(skl_eigenvectors - own_eigenvectors)\n",
    "print(\"Differences in eigenvectors: \\n\", check_df)\n",
    "\n",
    "# Check if values in the DataFrame are nearly zero\n",
    "is_close_to_zero = np.isclose(check_df, 0.0, atol=1e-5)\n",
    "print(\"Are all eigenvector differences close to zero? \", np.all(is_close_to_zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the eigenvectors from pca package\n",
    "pca_eigenvectors = pca_pca(n_components=2).fit_transform(train)['loadings']\n",
    "pca_eigenvectors = np.transpose(pca_eigenvectors)\n",
    "\n",
    "# Check if the eigenvectors contain the same values\n",
    "check_df = abs(pca_eigenvectors - own_eigenvectors)\n",
    "print(\"Differences in eigenvectors: \\n\", check_df)\n",
    "\n",
    "# Check if values in the DataFrame are nearly zero\n",
    "is_close_to_zero = np.isclose(check_df, 0.0, atol=1e-5)\n",
    "print(\"Are all eigenvector differences close to zero? \", np.all(is_close_to_zero))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Success!\n",
    "For two out of three libraries, the components are equivalent with ours! (Depends on the sample) Now let's see, how we can apply our components to transform the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new training sample\n",
    "data_sample = random_sample(df_labeled, 801, 2000, labeled=True)\n",
    "Y = pd.DataFrame(data_sample['Class'])\n",
    "Y.columns = ['Class']\n",
    "X = center_data(data_sample.drop(['Class'], axis=1))\n",
    "\n",
    "# Apply our own PCA\n",
    "eigenvalues, eigenvectors = our_pca(X, 2)\n",
    "eigenvectors = pd.DataFrame(eigenvectors)\n",
    "transformed_data = apply_components(X, eigenvectors)\n",
    "transformed_data = pd.DataFrame(transformed_data)\n",
    "\n",
    "# Add labels\n",
    "transformed_data.columns = ['pca1', 'pca2']\n",
    "transformed_data['cancer_type'] = Y['Class']\n",
    "\n",
    "# present the data on the 5 clusters using seaborn maps\n",
    "sns.scatterplot(x='pca1',y='pca2',hue = 'cancer_type', data=transformed_data)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1), title=\"Cancer Type\", frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "## 4. Use results for classification\n",
    "In this section, we will see, whether our extracted components can be used to classify subjects into different cancer types. First, we will do the classification without dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate classifiers\n",
    "classifier1 = DecisionTreeClassifier()\n",
    "classifier2 = DecisionTreeClassifier()\n",
    "classifier3 = DecisionTreeClassifier()\n",
    "\n",
    "# Split the transformed data set\n",
    "X = df_unlabeled\n",
    "y = df_labeled['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Decision Tree with original data\n",
    "classifier1.fit(X_train, y_train)\n",
    "predicted_y = classifier1.predict(X_test)\n",
    "\n",
    "# Evaluating the accuracy, precision, recall, f1\n",
    "accuracy = accuracy_score(y_test, predicted_y)\n",
    "precision = precision_score(y_test, predicted_y, average='macro')\n",
    "recall = recall_score(y_test, predicted_y, average='macro')\n",
    "f1 = 2*precision*recall / (precision+recall)\n",
    "print(\"Prediction success of the original features without dimension reduction\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see, if the classification works with the data transformed by the library PCA method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting subsample from data and defining X and y\n",
    "y = df_labeled['Class']\n",
    "X = center_data(df_unlabeled)\n",
    "\n",
    "# Split the transformed data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate sklearn pca\n",
    "skl_pca_model = skl_pca(n_components=2)\n",
    "transformed_data = skl_pca_model.fit_transform(X_train)\n",
    "\n",
    "# Decision Tree with transformed data\n",
    "classifier2.fit(transformed_data, y_train)\n",
    "\n",
    "# predict new data\n",
    "newdata = X_test.copy()\n",
    "\n",
    "# transform newdata using fitted pca\n",
    "newdata_transformed = skl_pca_model.transform(newdata)\n",
    "\n",
    "# predict labels with trained classifier\n",
    "predicted_y = classifier2.predict(newdata_transformed)\n",
    "\n",
    "# Evaluating the accuracy, precision, recall, f1\n",
    "accuracy = accuracy_score(y_test, predicted_y)\n",
    "precision = precision_score(y_test, predicted_y, average='macro')\n",
    "recall = recall_score(y_test, predicted_y, average='macro')\n",
    "f1 = 2*precision*recall / (precision+recall)\n",
    "print(\"Prediction success for all features, transformed with sklearn components:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compare the prediction accuracy with our own implementation. Note that the lower accuracy is mostly due to the reduced feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting sub sample from data and defining X and y\n",
    "data_sample = random_sample(df_labeled, 801, 2000, labeled=True)\n",
    "y = data_sample['Class']\n",
    "X = center_data(data_sample.drop(['Class'], axis=1))\n",
    "\n",
    "# Split the transformed data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Applying our own PCA\n",
    "eigenvalues, eigenvectors = our_pca(X_train, 2)\n",
    "transformed_data = apply_components(X_train, eigenvectors)\n",
    "\n",
    "# Fit the data\n",
    "classifier3.fit(transformed_data, y_train)\n",
    "# predict test data using fitted pca\n",
    "newdata_transformed = apply_components(X_test, eigenvectors)\n",
    "# predict labels with trained classifier\n",
    "pred_labels = classifier3.predict(newdata_transformed)\n",
    "\n",
    "# Evaluating the accuracy, precision, recall, f1\n",
    "accuracy = accuracy_score(y_test, pred_labels)\n",
    "precision = precision_score(y_test, pred_labels, average='macro')\n",
    "recall = recall_score(y_test, pred_labels, average='macro')\n",
    "f1 = 2*precision*recall / (precision+recall)\n",
    "print(\"Prediction success for only 2000 features, transformed with own components:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occurrences of each cancer type in the actual labels (y_test)\n",
    "unique_labels_test, label_counts_test = np.unique(y_test, return_counts=True)\n",
    "\n",
    "# Occurrences of each cancer type in the predicted labels (pred_labels)\n",
    "unique_labels_pred, label_counts_pred = np.unique(pred_labels, return_counts=True)\n",
    "\n",
    "# Unique cancer types\n",
    "unique_labels = np.unique(np.concatenate((unique_labels_test, unique_labels_pred)))\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "# x-axis positions\n",
    "index = np.arange(len(unique_labels))\n",
    "\n",
    "# Plot the bars for actual labels (y_test)\n",
    "plt.bar(index, label_counts_test, bar_width, label='Actual Labels', color='blue')\n",
    "\n",
    "# Plot the bars for predicted labels (pred_labels)\n",
    "plt.bar(index + bar_width, label_counts_pred, bar_width, label='Predicted Labels', color='green')\n",
    "\n",
    "# Set x-axis labels and tick positions\n",
    "plt.xlabel('Cancer Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Own PCA: Comparison of Actual and Predicted Labels')\n",
    "plt.xticks(index + bar_width / 2, unique_labels)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "## 5. Compare results with tSNE\n",
    "In this section, we will see, if we PCA was the right technique for us or whether another dimension reduction method (tSNE) would actually perform much better on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate t-SNE model\n",
    "tsne_model = TSNE(learning_rate=200, perplexity=50)\n",
    "tsne_data = df_unlabeled.copy()\n",
    "\n",
    "# t-SNE features are computed and a subset is printed\n",
    "tnse_features = tsne_model.fit_transform(tsne_data)\n",
    "\n",
    "# Two new columns 'x' and 'y' are added to the dataframe. They correspond to the t-SNE features.\n",
    "tsne_data['x'] = tnse_features[:,0]\n",
    "tsne_data['y'] = tnse_features[:,1]\n",
    "\n",
    "# The 'Class' column is added back to the dataframe under a new name 'cancer'\n",
    "tsne_data['cancer'] = df_labeled['Class']\n",
    "\n",
    "# The t-SNE features are plotted again, but this time the points are colored based on the 'cancer' class\n",
    "sns.scatterplot(x = 'x', y = 'y', hue = 'cancer', data = tsne_data)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the t-SNE features to classify our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate classifier\n",
    "classifier4 = DecisionTreeClassifier()\n",
    "\n",
    "# Split the transformed data set\n",
    "X = df_unlabeled\n",
    "y = df_labeled['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train new t-SNE model with train/test split\n",
    "tsne_model = TSNE(learning_rate=200, perplexity=50)\n",
    "tsne_data = X_train.copy()\n",
    "\n",
    "# t-SNE features are computed and a subset is printed\n",
    "tsne_features = tsne_model.fit_transform(tsne_data)\n",
    "\n",
    "# Decision Tree with data transformed by t-SNE features\n",
    "classifier4.fit(tsne_features, y_train)\n",
    "\n",
    "# predict test data using fitted pca\n",
    "tsne_data_test = X_test.copy()\n",
    "tsne_features_test = tsne_model.fit_transform(tsne_data_test)\n",
    "\n",
    "predicted_y = classifier4.predict(tsne_features_test)\n",
    "\n",
    "# Evaluating the accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_y)\n",
    "precision = precision_score(y_test, predicted_y, average='macro')\n",
    "recall = recall_score(y_test, predicted_y, average='macro')\n",
    "f1 = 2*precision*recall / (precision+recall)\n",
    "print(\"Prediction success of the original number of features, transformed with t-SNE features:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
